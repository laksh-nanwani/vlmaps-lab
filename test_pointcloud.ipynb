{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "__file__:  /home2/laksh.nanwani/vlmaps-lab/examples/context.py\n",
      "imported path: /home2/laksh.nanwani/vlmaps-lab\n"
     ]
    }
   ],
   "source": [
    "# @title Helper functions for VLMap Creation\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import clip\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from utils.clip_mapping_utils import load_semantic, load_obj2cls_dict, save_map, cvt_obj_id_2_cls_id, depth2pc_realsense, transform_pc, get_real_cam_mat, pos2grid_id, project_point, get_color\n",
    "# from utils.clip_mapping_utils import load_pose\n",
    "\n",
    "from lseg.modules.models.lseg_net import LSegEncNet\n",
    "from lseg.additional_utils.models import resize_image, pad_image, crop_image\n",
    "\n",
    "\n",
    "def load_depth(depth_filepath):\n",
    "    with open(depth_filepath, 'rb') as f:\n",
    "        depth = np.load(f)\n",
    "    return depth / 1000\n",
    "    # return depth\n",
    "\n",
    "def load_pose(pose_filepath):\n",
    "    with open(pose_filepath, 'rb') as f:\n",
    "        full_pose = np.load(f)\n",
    "        pos = np.array(full_pose[:3], dtype=float).reshape((3, 1))\n",
    "        quat = full_pose[3:]\n",
    "        # quat[[1,2]] = quat[[2,1]]\n",
    "        r = R.from_quat(quat)\n",
    "        rot = r.as_matrix()\n",
    "\n",
    "        return pos, rot\n",
    "\n",
    "# @title Helper functions for VLMap Creation\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.clip_mapping_utils import load_semantic, load_obj2cls_dict, save_map, cvt_obj_id_2_cls_id, depth2pc_realsense, transform_pc, get_real_cam_mat, pos2grid_id, project_point, get_color\n",
    "# from utils.clip_mapping_utils import load_pose\n",
    "\n",
    "from lseg.modules.models.lseg_net import LSegEncNet\n",
    "from lseg.additional_utils.models import resize_image, pad_image, crop_image\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "def create_lseg_map_batch(img_save_dir, camera_translation, ground_to_camera_height, cs=0.05, gs=1000, depth_sample_rate=100):\n",
    "    mask_version = 1 # 0, 1\n",
    "\n",
    "    # crop_size = 480 # 480\n",
    "    # base_size = 520 # 520\n",
    "    # lang = \"door,chair,ground,ceiling,window,other\"\n",
    "    # labels = lang.split(\",\")\n",
    "\n",
    "    # # loading models\n",
    "    # device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # print(device)\n",
    "    # clip_version = \"ViT-B/32\"\n",
    "    # clip_feat_dim = {'RN50': 1024, 'RN101': 512, 'RN50x4': 640, 'RN50x16': 768,\n",
    "    #                 'RN50x64': 1024, 'ViT-B/32': 512, 'ViT-B/16': 512, 'ViT-L/14': 768}[clip_version]\n",
    "    # print(\"Loading CLIP model...\")\n",
    "    # clip_model, preprocess = clip.load(clip_version, device = device)  # clip.available_models()\n",
    "    # clip_model.to(device).eval()\n",
    "    # # print(\"Clip model -\", clip_model.is_cuda())\n",
    "    # # x = torch.randn((5,5)).to(device=device)\n",
    "    # # print(\"Tensor device - \", type(x))\n",
    "    # # clip_model.eval()\n",
    "    # lang_token = clip.tokenize(labels)\n",
    "    # lang_token = lang_token.to(device)\n",
    "    # with torch.no_grad():\n",
    "    #     text_feats = clip_model.encode_text(lang_token)\n",
    "    #     text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
    "    # text_feats = text_feats.cpu().numpy()\n",
    "    # model = LSegEncNet(lang, arch_option=0,\n",
    "    #                     block_depth=0,\n",
    "    #                     activation='lrelu',\n",
    "    #                     crop_size=crop_size)\n",
    "    # model_state_dict = model.state_dict()\n",
    "    # global data_root\n",
    "    # checkpoint_path = os.path.join(data_root, \"lseg/checkpoints/demo_e200.ckpt\")\n",
    "\n",
    "    # print(\"Loading lseg checkpoint\")\n",
    "    # pretrained_state_dict = torch.load(checkpoint_path, map_location = device)\n",
    "    # print(\"Lseg loaded\")\n",
    "    # pretrained_state_dict = {k.lstrip('net.'): v for k, v in pretrained_state_dict['state_dict'].items()}\n",
    "    # model_state_dict.update(pretrained_state_dict)\n",
    "    # model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "    # model.eval()\n",
    "    # model = model.cuda()\n",
    "\n",
    "    # norm_mean= [0.5, 0.5, 0.5]\n",
    "    # norm_std = [0.5, 0.5, 0.5]\n",
    "    # padding = [0.0] * 3\n",
    "    # transform = transforms.Compose(\n",
    "    #     [\n",
    "    #         transforms.ToTensor(),\n",
    "    #         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    print(f\"loading scene {img_save_dir}\")\n",
    "    rgb_dir = os.path.join(img_save_dir, \"rgb\")\n",
    "    depth_dir = os.path.join(img_save_dir, \"depth\")\n",
    "    pose_dir = os.path.join(img_save_dir, \"pose\")\n",
    "    # semantic_dir = os.path.join(img_save_dir, \"semantic\")\n",
    "    # obj2cls_path = os.path.join(img_save_dir, \"obj2cls_dict.txt\")\n",
    "\n",
    "    rgb_list = sorted(os.listdir(rgb_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    depth_list = sorted(os.listdir(depth_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    # pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "    #     x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    # semantic_list = sorted(os.listdir(semantic_dir), key=lambda x: int(\n",
    "    #     x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "    rgb_list = [os.path.join(rgb_dir, x) for x in rgb_list]\n",
    "    depth_list = [os.path.join(depth_dir, x) for x in depth_list]\n",
    "    pose_list = [os.path.join(pose_dir, x) for x in pose_list]\n",
    "    # semantic_list = [os.path.join(semantic_dir, x) for x in semantic_list]\n",
    "\n",
    "\n",
    "    map_save_dir = os.path.join(img_save_dir, \"map\")\n",
    "    os.makedirs(map_save_dir, exist_ok=True)\n",
    "    # color_top_down_save_path = os.path.join(map_save_dir, f\"color_top_down_{mask_version}.npy\")\n",
    "    # # gt_save_path = os.path.join(map_save_dir, f\"grid_{mask_version}_gt.npy\")\n",
    "    # grid_save_path = os.path.join(map_save_dir, f\"grid_lseg_{mask_version}.npy\")\n",
    "    # weight_save_path = os.path.join(map_save_dir, f\"weight_lseg_{mask_version}.npy\")\n",
    "    # obstacles_save_path = os.path.join(map_save_dir, \"obstacles.npy\")\n",
    "\n",
    "    # obj2cls = load_obj2cls_dict(obj2cls_path)\n",
    "\n",
    "    # initialize a grid with zero position at the center\n",
    "    color_top_down_height = (camera_translation[2] + 1) * np.ones((gs, gs), dtype=np.float32)\n",
    "    color_top_down = np.zeros((gs, gs, 3), dtype=np.uint8)\n",
    "    # gt = np.zeros((gs, gs), dtype=np.int32)\n",
    "    # grid = np.zeros((gs, gs, clip_feat_dim), dtype=np.float32)\n",
    "    obstacles = np.ones((gs, gs), dtype=np.uint8)\n",
    "    weight = np.zeros((gs, gs), dtype=float)\n",
    "\n",
    "    # save_map(color_top_down_save_path, color_top_down)\n",
    "    # # save_map(gt_save_path, gt)\n",
    "    # save_map(grid_save_path, grid)\n",
    "    # save_map(weight_save_path, weight)\n",
    "    # save_map(obstacles_save_path, obstacles)\n",
    "\n",
    "    tf_list = []\n",
    "    # data_iter = zip(rgb_list, depth_list, semantic_list, pose_list)\n",
    "    data_iter = zip(rgb_list, depth_list, pose_list)\n",
    "    pbar = tqdm(total=len(rgb_list))\n",
    "    # load all images and depths and poses\n",
    "    # print('here')\n",
    "    count = 0\n",
    "\n",
    "    trans_cam = np.zeros((4,4))\n",
    "    trans_cam[0, 2] = 1\n",
    "    trans_cam[1, 0] = -1\n",
    "    trans_cam[2, 1] = -1\n",
    "\n",
    "    trans_cam[:3,3] = np.array(camera_translation)\n",
    "    trans_cam[3,3] = 1\n",
    "\n",
    "    pc_combined = []\n",
    "    colors = []\n",
    "    # feature_cloud = []\n",
    "    \n",
    "    for data_sample in data_iter:\n",
    "        count += 1\n",
    "        # if count % 15 != 0:\n",
    "        #     continue\n",
    "\n",
    "        feature_list = []\n",
    "        pixel_coords = []\n",
    "        # count += 1\n",
    "        # if count % 3 != 0:\n",
    "        #     continue\n",
    "        # rgb_path, depth_path, semantic_path, pose_path = data_sample\n",
    "        rgb_path, depth_path, pose_path = data_sample\n",
    "\n",
    "        \n",
    "        bgr = cv2.imread(rgb_path)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        out = rgb.copy()\n",
    "\n",
    "        # read pose\n",
    "        # pos, rot = load_pose(pose_path)  # z backward, y upward, x to the right -> theirs\n",
    "        pos, rot = load_pose(pose_path)  # x front, y left, z upward -> ours\n",
    "\n",
    "        pose = np.eye(4)\n",
    "        pose[:3, :3] = rot\n",
    "        pose[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "        # pose = pose @ trans_cam\n",
    "\n",
    "        tf_list.append(pose)\n",
    "        if len(tf_list) == 1:\n",
    "            init_tf_inv = np.linalg.inv(tf_list[0])\n",
    "\n",
    "        tf = init_tf_inv @ pose @ trans_cam\n",
    "        # tf = init_tf_inv @ pose   # uncomment, if pose = pose @ trans_cam\n",
    "\n",
    "        # read depth\n",
    "        depth = load_depth(depth_path)\n",
    "        # print(f\"depth shape = {depth.shape}\")\n",
    "        # break\n",
    "        # read semantic\n",
    "        # semantic = load_semantic(semantic_path)\n",
    "        # semantic = cvt_obj_id_2_cls_id(semantic, obj2cls)\n",
    "\n",
    "        # pix_feats = get_lseg_feat(model, rgb, labels, transform, crop_size, base_size, norm_mean, norm_std)\n",
    "        # print(pix_feats.shape)\n",
    "        # transform all points to the global frame\n",
    "        pc, mask = depth2pc_realsense(depth)\n",
    "        \n",
    "        shuffle_mask = np.arange(pc.shape[1]) \n",
    "        np.random.shuffle(shuffle_mask)\n",
    "        shuffle_mask = shuffle_mask[::depth_sample_rate]\n",
    "        mask = mask[shuffle_mask]\n",
    "        pc = pc[:, shuffle_mask]\n",
    "        pc = pc[:, mask]\n",
    "\n",
    "        color_local = get_color(rgb)\n",
    "        color_local = color_local[:, shuffle_mask]\n",
    "        color_local = color_local[:, mask]\n",
    "\n",
    "\n",
    "        # o3d_pc = o3d.geometry.PointCloud()\n",
    "        # o3d_pc.points = o3d.utility.Vector3dVector(pc.T)\n",
    "        # o3d_pc.colors = o3d.utility.Vector3dVector((color_local / 255))\n",
    "        # o3d_pc, _ = o3d_pc.remove_statistical_outlier(nb_neighbors = 15, std_ratio = 1.0)\n",
    "        # pc = (np.asarray(o3d_pc.points)).T\n",
    "        # color_local = (np.asarray(o3d_pc.colors)).T\n",
    "\n",
    "        pc_global = transform_pc(pc, tf)\n",
    "\n",
    "        # original_dims = (rgb.shape[1], rgb.shape[0])\n",
    "        # rgb_cam_mat = get_real_cam_mat(original_dims, original_dims)\n",
    "        # print(rgb_cam_mat)\n",
    "        # feat_cam_mat = get_real_cam_mat(original_dims, (pix_feats.shape[3], pix_feats.shape[2]))\n",
    "        # print(feat_cam_mat)\n",
    "\n",
    "        if len(pc_combined) == 0:\n",
    "            pc_combined = pc_global\n",
    "            colors = color_local\n",
    "        else:\n",
    "            pc_combined = np.hstack((pc_combined, pc_global))\n",
    "            # print(color_local.shape, colors.shape)\n",
    "            colors = np.hstack((colors, color_local))\n",
    "            \n",
    "        # print(f\"pc_local shape {pc.shape}\")\n",
    "        # project all point cloud onto the ground\n",
    "        # print(np.max(pc[1]))\n",
    "        # for i, (p, p_local) in enumerate(zip(pc_global.T, pc.T)):\n",
    "            # x, y = pos2grid_id(gs, cs, p[0], p[2])   # if pose = pose @ trans_cam\n",
    "            # x, y = pos2grid_id(gs, cs, p[0], p[1])   # changed p[2] -> p[1], i.e, z -> y\n",
    "\n",
    "            # rgb_px, rgb_py, rgb_pz = project_point(rgb_cam_mat, p_local)\n",
    "            # rgb_v = rgb[rgb_py, rgb_px, :]\n",
    "            # colors.append(rgb_v)\n",
    "\n",
    "            # ignore points projected to outside of the map and points that are 1.0 higher than the camera (could be from the ceiling)\n",
    "            # if x >= obstacles.shape[0] or y >= obstacles.shape[1] or \\\n",
    "            #     x < 0 or y < 0 or p_local[1] < -1.0:\n",
    "            #     continue\n",
    "\n",
    "            # semantic_v = semantic[rgb_py, rgb_px]\n",
    "            # if semantic_v == 40:\n",
    "            #     semantic_v = -1\n",
    "            \n",
    "            # when the projected location is already assigned a color value before, overwrite if the current point has larger height\n",
    "            # if p_local[1] < color_top_down_height[y, x]:\n",
    "            #     color_top_down[y, x] = rgb_v\n",
    "            #     color_top_down_height[y, x] = p_local[1]\n",
    "                # gt[y, x] = semantic_v\n",
    "\n",
    "            # average the visual embeddings if multiple points are projected to the same grid cell\n",
    "            # px, py, pz = project_point(feat_cam_mat, p_local)\n",
    "            # if not (px < 0 or py < 0 or px >= pix_feats.shape[3] or py >= pix_feats.shape[2]):\n",
    "                # feat = pix_feats[0, :, py, px]\n",
    "                # print(feat.shape)\n",
    "                # feature_cloud.append(feat)\n",
    "                # grid[y, x] = (grid[y, x] * weight[y, x] + feat) / (weight[y, x] + 1)\n",
    "                # print(grid[y, x].shape)\n",
    "                # weight[y, x] += 1\n",
    "                \n",
    "                # if count == 1:\n",
    "                # pixel_coords.append([rgb_py, rgb_px])\n",
    "                # feature_list.append(feat)\n",
    "                # out = cv2.circle(out, (rgb_px, rgb_py), radius = 2, color = (255, 0, 0), thickness = 1)\n",
    "            \n",
    "            # build an obstacle map ignoring points on the floor (0 means occupied, 1 means free)\n",
    "            # if p_local[1] > ground_to_camera_height:\n",
    "            #     continue\n",
    "            # obstacles[y, x] = 0\n",
    "\n",
    "        # plt.imshow(out)\n",
    "        # plt.show()\n",
    "        pbar.update(1)\n",
    "        np.save(os.path.join(map_save_dir, \"pointcloud.npy\"), pc_combined)\n",
    "        np.save(os.path.join(map_save_dir, \"colors.npy\"), np.array(colors))\n",
    "        # count += 1\n",
    "        # break\n",
    "        # if count == 75:\n",
    "        #     break\n",
    "        # print(len(pixel_coords))\n",
    "        # print(feature_list.shape)\n",
    "\n",
    "    # pc_combined = pc_combined @ trans_cam\n",
    "    colors = np.array(colors)\n",
    "    # pixel_coords = np.array(pixel_coords)\n",
    "    # feature_list = np.array(feature_list)   \n",
    "    # save_map(color_top_down_save_path, color_top_down)\n",
    "    # save_map(gt_save_path, gt)\n",
    "    # save_map(grid_save_path, grid)\n",
    "    # save_map(weight_save_path, weight)\n",
    "    # save_map(obstacles_save_path, obstacles)\n",
    "    # np.save(os.path.join(map_save_dir, \"pixel_coords.npy\"), pixel_coords)\n",
    "    # np.save(os.path.join(map_save_dir, \"features_list.npy\"), feature_list)\n",
    "    # print(pixel_coords.shape)\n",
    "    # print(feature_list.shape)\n",
    "    np.save(os.path.join(map_save_dir, \"pointcloud.npy\"), pc_combined)\n",
    "    np.save(os.path.join(map_save_dir, \"colors.npy\"), colors)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2,3], [4,5,6]])[np.array([0,0]), np.array([1,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home2/laksh.nanwani/vlmaps-lab']\n",
      "/home2/laksh.nanwani/vlmaps-lab\n",
      "/scratch/laksh.nanwani/vlmaps_data/data_synced\n"
     ]
    }
   ],
   "source": [
    "root_dir = !pwd\n",
    "print(root_dir)\n",
    "root_dir = root_dir[0]\n",
    "print(root_dir)\n",
    "# !mkdir data\n",
    "# %cd data\n",
    "# !gdown 1wjuiVcO92Rqer5gLk-X7hINfe4PCMQmu\n",
    "# !pip install tqdm\n",
    "# !unzip -o 5LpN3gDmAk7_1.zip | tqdm --desc extracted --unit files --unit_scale --total `unzip -l 5LpN3gDmAk7_1.zip | tail -n 1 | xargs echo -n | cut -d' ' -f2` > /dev/null\n",
    "import os\n",
    "data_root = os.path.join(root_dir.split(\"home2\")[0], \"scratch/laksh.nanwani/vlmaps_data\")\n",
    "\n",
    "# checkpoint_dir = os.path.join(data_root, \"checkpoints\")\n",
    "folder_name = \"data_synced\"\n",
    "data_dir = os.path.join(data_root, folder_name)\n",
    "# data_dir = os.path.join(data_root, \"data_depth_issue\")\n",
    "\n",
    "\n",
    "# print(checkpoint_dir)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "# @markdown meters per cell size\n",
    "cs = 0.05 # @param {type: \"number\"}\n",
    "# @markdown map resolution (gs x gs)\n",
    "gs = 1000 # @param {type: \"integer\"}\n",
    "# @markdown camera height (used for filtering out points on the floor)\n",
    "# camera_height = 1.5 # @param {type: \"number\"}\n",
    "# 0.51 - base_link -> camera_link\n",
    "# camera_translation = [0.2, -0.222, 0.512]\n",
    "camera_translation = [0.1, 0.018, 0.55]   # p3dx base_link to camera_link\n",
    "# @markdown depth pixels subsample rate\n",
    "depth_sample_rate = 100 # @param {type: \"integer\"}\n",
    "# @markdown data where rgb, depth, pose are loaded and map are saved\n",
    "data_dir = data_dir # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scene /scratch/laksh.nanwani/vlmaps_data/data_synced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 50/3403 [00:04<04:45, 11.76it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pcd, colors \u001b[39m=\u001b[39m create_lseg_map_batch(data_dir, camera_translation\u001b[39m=\u001b[39;49mcamera_translation, ground_to_camera_height \u001b[39m=\u001b[39;49m \u001b[39m0.5\u001b[39;49m, cs\u001b[39m=\u001b[39;49mcs, gs\u001b[39m=\u001b[39;49mgs, depth_sample_rate\u001b[39m=\u001b[39;49mdepth_sample_rate)\n",
      "Cell \u001b[0;32mIn[10], line 198\u001b[0m, in \u001b[0;36mcreate_lseg_map_batch\u001b[0;34m(img_save_dir, camera_translation, ground_to_camera_height, cs, gs, depth_sample_rate)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m# count += 1\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m# if count % 3 != 0:\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m#     continue\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m# rgb_path, depth_path, semantic_path, pose_path = data_sample\u001b[39;00m\n\u001b[1;32m    195\u001b[0m rgb_path, depth_path, pose_path \u001b[39m=\u001b[39m data_sample\n\u001b[0;32m--> 198\u001b[0m bgr \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mimread(rgb_path)\n\u001b[1;32m    199\u001b[0m rgb \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(bgr, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m    200\u001b[0m out \u001b[39m=\u001b[39m rgb\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 51/3403 [00:20<04:44, 11.76it/s]"
     ]
    }
   ],
   "source": [
    "pcd, colors = create_lseg_map_batch(data_dir, camera_translation=camera_translation, ground_to_camera_height = 0.5, cs=cs, gs=gs, depth_sample_rate=depth_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 399606)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd.shape\n",
    "# colors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] GLFW Error: X11: The DISPLAY environment variable is missing\u001b[0;m\n",
      "\u001b[1;33m[Open3D WARNING] Failed to initialize GLFW\u001b[0;m\n",
      "\u001b[1;33m[Open3D WARNING] [DrawGeometries] Failed creating OpenGL window.\u001b[0;m\n"
     ]
    }
   ],
   "source": [
    "PC = o3d.geometry.PointCloud()\n",
    "PC.points = o3d.utility.Vector3dVector(pcd.T)\n",
    "o3d.visualization.draw_geometries([PC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home2/laksh.nanwani/vlmaps-lab']\n",
      "/home2/laksh.nanwani/vlmaps-lab\n",
      "/scratch/laksh.nanwani/vlmaps_data/5LpN3gDmAk7_1\n"
     ]
    }
   ],
   "source": [
    "root_dir = !pwd\n",
    "print(root_dir)\n",
    "root_dir = root_dir[0]\n",
    "print(root_dir)\n",
    "# !mkdir data\n",
    "# %cd data\n",
    "# !gdown 1wjuiVcO92Rqer5gLk-X7hINfe4PCMQmu\n",
    "# !pip install tqdm\n",
    "# !unzip -o 5LpN3gDmAk7_1.zip | tqdm --desc extracted --unit files --unit_scale --total `unzip -l 5LpN3gDmAk7_1.zip | tail -n 1 | xargs echo -n | cut -d' ' -f2` > /dev/null\n",
    "import os\n",
    "data_root = os.path.join(root_dir.split(\"home2\")[0], \"scratch/laksh.nanwani/vlmaps_data\")\n",
    "\n",
    "# checkpoint_dir = os.path.join(data_root, \"checkpoints\")\n",
    "data_dir = os.path.join(data_root, \"5LpN3gDmAk7_1\")\n",
    "\n",
    "# print(checkpoint_dir)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "# @markdown meters per cell size\n",
    "cs = 0.05 # @param {type: \"number\"}\n",
    "# @markdown map resolution (gs x gs)\n",
    "gs = 1000 # @param {type: \"integer\"}\n",
    "# @markdown camera height (used for filtering out points on the floor)\n",
    "camera_height = 1.5 # @param {type: \"number\"}\n",
    "# @markdown depth pixels subsample rate\n",
    "depth_sample_rate = 50 # @param {type: \"integer\"}\n",
    "# @markdown data where rgb, depth, pose are loaded and map are saved\n",
    "data_dir = data_dir # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__file__:  /home2/laksh.nanwani/vlmaps-lab/examples/context.py\n",
      "imported path: /home2/laksh.nanwani/vlmaps-lab\n"
     ]
    }
   ],
   "source": [
    "from utils.clip_mapping_utils import depth2pc as depth2pc_demo\n",
    "from utils.clip_mapping_utils import load_pose as load_pose_demo\n",
    "\n",
    "def load_depth_demo(depth_filepath):\n",
    "    with open(depth_filepath, 'rb') as f:\n",
    "        depth = np.load(f)\n",
    "    return depth\n",
    "\n",
    "def create_lseg_map_batch_demo(img_save_dir, camera_height, cs=0.05, gs=1000, depth_sample_rate=100):\n",
    "    mask_version = 1 # 0, 1\n",
    "\n",
    "    print(f\"loading scene {img_save_dir}\")\n",
    "    rgb_dir = os.path.join(img_save_dir, \"rgb\")\n",
    "    depth_dir = os.path.join(img_save_dir, \"depth\")\n",
    "    pose_dir = os.path.join(img_save_dir, \"pose\")\n",
    "    semantic_dir = os.path.join(img_save_dir, \"semantic\")\n",
    "    # obj2cls_path = os.path.join(img_save_dir, \"obj2cls_dict.txt\")\n",
    "\n",
    "    rgb_list = sorted(os.listdir(rgb_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    depth_list = sorted(os.listdir(depth_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    semantic_list = sorted(os.listdir(semantic_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "    rgb_list = [os.path.join(rgb_dir, x) for x in rgb_list]\n",
    "    depth_list = [os.path.join(depth_dir, x) for x in depth_list]\n",
    "    pose_list = [os.path.join(pose_dir, x) for x in pose_list]\n",
    "    semantic_list = [os.path.join(semantic_dir, x) for x in semantic_list]\n",
    "\n",
    "\n",
    "    map_save_dir = os.path.join(img_save_dir, \"map_demo\")\n",
    "    os.makedirs(map_save_dir, exist_ok=True)\n",
    "\n",
    "    tf_list = []\n",
    "    data_iter = zip(rgb_list, depth_list, semantic_list, pose_list)\n",
    "    pbar = tqdm(total=len(rgb_list))\n",
    "    # load all images and depths and poses\n",
    "    print('here')\n",
    "\n",
    "    pc_combined = []\n",
    "    colors = []\n",
    "    \n",
    "    for data_sample in data_iter:\n",
    "        rgb_path, depth_path, semantic_path, pose_path = data_sample\n",
    "        \n",
    "        bgr = cv2.imread(rgb_path)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        # out = rgb.copy()\n",
    "\n",
    "        # read pose\n",
    "        pos, rot = load_pose_demo(pose_path)  # z backward, y upward, x to the right\n",
    "        rot_ro_cam = np.eye(3)\n",
    "        rot_ro_cam[1, 1] = -1\n",
    "        rot_ro_cam[2, 2] = -1\n",
    "        rot = rot @ rot_ro_cam\n",
    "        pos[1] += camera_height\n",
    "\n",
    "\n",
    "        pose = np.eye(4)\n",
    "        pose[:3, :3] = rot\n",
    "        pose[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "        tf_list.append(pose)\n",
    "        if len(tf_list) == 1:\n",
    "            init_tf_inv = np.linalg.inv(tf_list[0]) \n",
    "\n",
    "        tf = init_tf_inv @ pose\n",
    "\n",
    "        # read depth\n",
    "        depth = load_depth_demo(depth_path)\n",
    "        # print(f\"depth shape = {depth.shape}\")\n",
    "        \n",
    "        # transform all points to the global frame\n",
    "        pc, mask = depth2pc_demo(depth)\n",
    "        print(pc.shape)\n",
    "\n",
    "        # o3d.visualization.draw_geometries([pc])\n",
    "        \n",
    "        shuffle_mask = np.arange(pc.shape[1])\n",
    "        np.random.shuffle(shuffle_mask)\n",
    "        shuffle_mask = shuffle_mask[::depth_sample_rate]\n",
    "        mask = mask[shuffle_mask]\n",
    "        pc = pc[:, shuffle_mask]\n",
    "        pc = pc[:, mask]\n",
    "        print(pc.shape)\n",
    "\n",
    "        color_local = get_color(rgb)\n",
    "        color_local = color_local[:, shuffle_mask]\n",
    "        color_local = color_local[:, mask]\n",
    "\n",
    "        pc_global = transform_pc(pc, tf)\n",
    "\n",
    "        if len(pc_combined) == 0:\n",
    "            pc_combined = pc_global\n",
    "            colors = color_local\n",
    "        else:\n",
    "            pc_combined = np.hstack((pc_combined, pc_global))\n",
    "            # print(color_local.shape, colors.shape)\n",
    "            colors = np.hstack((colors, color_local))\n",
    "\n",
    "        pbar.update(1)\n",
    "        np.save(os.path.join(map_save_dir, \"pointcloud.npy\"), pc_combined)\n",
    "        np.save(os.path.join(map_save_dir, \"colors.npy\"), np.array(colors))\n",
    "        # break\n",
    "\n",
    "    np.save(os.path.join(map_save_dir, \"pointcloud.npy\"), pc_combined)\n",
    "    np.save(os.path.join(map_save_dir, \"colors.npy\"), np.array(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scene /scratch/laksh.nanwani/vlmaps_data/5LpN3gDmAk7_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n",
      "(3, 777600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n",
      "(3, 777600)\n",
      "(3, 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_lseg_map_batch_demo(data_dir, camera_height\u001b[39m=\u001b[39;49mcamera_height, cs\u001b[39m=\u001b[39;49mcs, gs\u001b[39m=\u001b[39;49mgs, depth_sample_rate\u001b[39m=\u001b[39;49mdepth_sample_rate)\n",
      "Cell \u001b[0;32mIn[14], line 87\u001b[0m, in \u001b[0;36mcreate_lseg_map_batch_demo\u001b[0;34m(img_save_dir, camera_height, cs, gs, depth_sample_rate)\u001b[0m\n\u001b[1;32m     84\u001b[0m pc \u001b[39m=\u001b[39m pc[:, mask]\n\u001b[1;32m     85\u001b[0m \u001b[39mprint\u001b[39m(pc\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 87\u001b[0m color_local \u001b[39m=\u001b[39m get_color(rgb)\n\u001b[1;32m     88\u001b[0m color_local \u001b[39m=\u001b[39m color_local[:, shuffle_mask]\n\u001b[1;32m     89\u001b[0m color_local \u001b[39m=\u001b[39m color_local[:, mask]\n",
      "File \u001b[0;32m~/vlmaps-lab/utils/clip_mapping_utils.py:508\u001b[0m, in \u001b[0;36mget_color\u001b[0;34m(rgb)\u001b[0m\n\u001b[1;32m    506\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    507\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 508\u001b[0m colors \u001b[39m=\u001b[39m rgb[y,x]\n\u001b[1;32m    509\u001b[0m \u001b[39m# print(colors.shape)\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[39mreturn\u001b[39;00m colors\u001b[39m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_lseg_map_batch_demo(data_dir, camera_height=camera_height, cs=cs, gs=gs, depth_sample_rate=depth_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home2/laksh.nanwani/vlmaps-lab']\n",
      "/home2/laksh.nanwani/vlmaps-lab\n",
      "/scratch/laksh.nanwani/vlmaps_data/FloorPlan203\n"
     ]
    }
   ],
   "source": [
    "root_dir = !pwd\n",
    "print(root_dir)\n",
    "root_dir = root_dir[0]\n",
    "print(root_dir)\n",
    "# !mkdir data\n",
    "# %cd data\n",
    "# !gdown 1wjuiVcO92Rqer5gLk-X7hINfe4PCMQmu\n",
    "# !pip install tqdm\n",
    "# !unzip -o 5LpN3gDmAk7_1.zip | tqdm --desc extracted --unit files --unit_scale --total `unzip -l 5LpN3gDmAk7_1.zip | tail -n 1 | xargs echo -n | cut -d' ' -f2` > /dev/null\n",
    "import os\n",
    "data_root = os.path.join(root_dir.split(\"home2\")[0], \"scratch/laksh.nanwani/vlmaps_data\")\n",
    "\n",
    "# checkpoint_dir = os.path.join(data_root, \"checkpoints\")\n",
    "data_dir = os.path.join(data_root, \"FloorPlan203\")\n",
    "\n",
    "# print(checkpoint_dir)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "# @markdown meters per cell size\n",
    "cs = 0.05 # @param {type: \"number\"}\n",
    "# @markdown map resolution (gs x gs)\n",
    "gs = 1000 # @param {type: \"integer\"}\n",
    "# @markdown camera height (used for filtering out points on the floor)\n",
    "camera_height = 0.7 # @param {type: \"number\"}\n",
    "# @markdown depth pixels subsample rate\n",
    "depth_sample_rate = 50 # @param {type: \"integer\"}\n",
    "# @markdown data where rgb, depth, pose are loaded and map are saved\n",
    "data_dir = data_dir # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clip_mapping_utils import depth2pc_ai2thor #, load_poseai2thor_personal\n",
    "from utils.clip_mapping_utils import load_pose\n",
    "\n",
    "def load_depth_demo(depth_filepath):\n",
    "    with open(depth_filepath, 'rb') as f:\n",
    "        depth = np.load(f)\n",
    "    return depth\n",
    "\n",
    "def create_lseg_map_batch_ai2thor(img_save_dir, camera_height, cs=0.05, gs=1000, depth_sample_rate=100):\n",
    "    mask_version = 1 # 0, 1\n",
    "\n",
    "    print(f\"loading scene {img_save_dir}\")\n",
    "    rgb_dir = os.path.join(img_save_dir, \"rgb\")\n",
    "    depth_dir = os.path.join(img_save_dir, \"depth\")\n",
    "    pose_dir = os.path.join(img_save_dir, \"pose\")\n",
    "    # semantic_dir = os.path.join(img_save_dir, \"semantic\")\n",
    "    # obj2cls_path = os.path.join(img_save_dir, \"obj2cls_dict.txt\")\n",
    "\n",
    "    rgb_list = sorted(os.listdir(rgb_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    depth_list = sorted(os.listdir(depth_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    # semantic_list = sorted(os.listdir(semantic_dir), key=lambda x: int(\n",
    "    #     x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "    rgb_list = [os.path.join(rgb_dir, x) for x in rgb_list]\n",
    "    depth_list = [os.path.join(depth_dir, x) for x in depth_list]\n",
    "    pose_list = [os.path.join(pose_dir, x) for x in pose_list]\n",
    "    # semantic_list = [os.path.join(semantic_dir, x) for x in semantic_list]\n",
    "\n",
    "\n",
    "    map_save_dir = os.path.join(img_save_dir, \"map_demo\")\n",
    "    os.makedirs(map_save_dir, exist_ok=True)\n",
    "\n",
    "    tf_list = []\n",
    "    data_iter = zip(rgb_list, depth_list, pose_list)\n",
    "    pbar = tqdm(total=len(rgb_list))\n",
    "    # load all images and depths and poses\n",
    "    print('here')\n",
    "\n",
    "    pc_combined = []\n",
    "    colors = []\n",
    "    \n",
    "    count = 0\n",
    "    for data_sample in data_iter:\n",
    "        rgb_path, depth_path, pose_path = data_sample\n",
    "        # print(data_sample)\n",
    "        \n",
    "        bgr = cv2.imread(rgb_path)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        # out = rgb.copy()\n",
    "\n",
    "        # read pose\n",
    "        pos, rot = load_pose(pose_path)  # z backward, y upward, x to the right\n",
    "        rot_ro_cam = np.eye(3)\n",
    "        rot_ro_cam[1, 1] = -1\n",
    "        rot_ro_cam[2, 2] = -1\n",
    "        rot = rot @ rot_ro_cam\n",
    "        pos[1] += camera_height\n",
    "\n",
    "        # rot = rot.T @ rot_ro_cam\n",
    "        # pos[2] = -pos[2]\n",
    "\n",
    "        pose = np.eye(4)\n",
    "        pose[:3, :3] = rot\n",
    "        pose[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "        tf_list.append(pose)\n",
    "        if len(tf_list) == 1:\n",
    "            init_tf_inv = np.linalg.inv(tf_list[0]) \n",
    "\n",
    "        tf = init_tf_inv @ pose\n",
    "\n",
    "        # read depth\n",
    "        depth = load_depth_demo(depth_path)\n",
    "        # print(f\"depth shape = {depth.shape}\")\n",
    "        \n",
    "        # transform all points to the global frame\n",
    "        pc, mask = depth2pc_ai2thor(depth)\n",
    "        # print(pc.shape)\n",
    "\n",
    "        # o3d.visualization.draw_geometries([pc])\n",
    "        \n",
    "        shuffle_mask = np.arange(pc.shape[1])\n",
    "        np.random.shuffle(shuffle_mask)\n",
    "        shuffle_mask = shuffle_mask[::depth_sample_rate]\n",
    "        mask = mask[shuffle_mask]\n",
    "        pc = pc[:, shuffle_mask]\n",
    "        pc = pc[:, mask]\n",
    "        # print(pc.shape)\n",
    "\n",
    "        color_local = get_color(rgb)\n",
    "        color_local = color_local[:, shuffle_mask]\n",
    "        color_local = color_local[:, mask]\n",
    "\n",
    "        pc_global = transform_pc(pc, tf)\n",
    "\n",
    "        if len(pc_combined) == 0:\n",
    "            pc_combined = pc_global\n",
    "            colors = color_local\n",
    "        else:\n",
    "            pc_combined = np.hstack((pc_combined, pc_global))\n",
    "            # print(color_local.shape, colors.shape)\n",
    "            colors = np.hstack((colors, color_local))\n",
    "\n",
    "        pbar.update(1)\n",
    "        np.save(os.path.join(map_save_dir, \"pointcloud.npy\"), pc_combined)\n",
    "        np.save(os.path.join(map_save_dir, \"colors.npy\"), np.array(colors))\n",
    "        # break\n",
    "\n",
    "        if count == 10:\n",
    "            break\n",
    "        count += 1\n",
    "\n",
    "    np.save(os.path.join(map_save_dir, \"pointcloud.npy\"), pc_combined)\n",
    "    np.save(os.path.join(map_save_dir, \"colors.npy\"), np.array(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scene /scratch/laksh.nanwani/vlmaps_data/FloorPlan203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/986 [00:00<01:02, 15.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/986 [00:00<01:22, 11.75it/s]\n"
     ]
    }
   ],
   "source": [
    "create_lseg_map_batch_ai2thor(data_dir, camera_height=camera_height, cs=cs, gs=gs, depth_sample_rate=depth_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4411e80da8008d6a0c6bf425013b61b6956630bc6111cfb4a65d9f725bf5dc68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
