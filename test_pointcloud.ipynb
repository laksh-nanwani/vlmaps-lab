{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "__file__:  /home2/laksh.nanwani/vlmaps-lab/examples/context.py\n",
      "imported path: /home2/laksh.nanwani/vlmaps-lab\n"
     ]
    }
   ],
   "source": [
    "# @title Helper functions for VLMap Creation\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import clip\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from utils.clip_mapping_utils import load_semantic, load_obj2cls_dict, save_map, cvt_obj_id_2_cls_id, depth2pc_realsense, transform_pc, get_real_cam_mat, pos2grid_id, project_point, get_color\n",
    "# from utils.clip_mapping_utils import load_pose\n",
    "\n",
    "from lseg.modules.models.lseg_net import LSegEncNet\n",
    "from lseg.additional_utils.models import resize_image, pad_image, crop_image\n",
    "\n",
    "\n",
    "def load_depth(depth_filepath):\n",
    "    with open(depth_filepath, 'rb') as f:\n",
    "        depth = np.load(f)\n",
    "    return depth / 1000\n",
    "    # return depth\n",
    "\n",
    "def load_pose(pose_filepath):\n",
    "    with open(pose_filepath, 'rb') as f:\n",
    "        full_pose = np.load(f)\n",
    "        pos = np.array(full_pose[:3], dtype=float).reshape((3, 1))\n",
    "        quat = full_pose[3:]\n",
    "        # quat[[1,2]] = quat[[2,1]]\n",
    "        r = R.from_quat(quat)\n",
    "        rot = r.as_matrix()\n",
    "\n",
    "        return pos, rot\n",
    "\n",
    "# def get_color(rgb):\n",
    "#     h,w = rgb.shape[:2]\n",
    "#     y, x = np.meshgrid(np.arange(h), np.arange(w), indexing=\"ij\")\n",
    "\n",
    "#     x = x.reshape((-1))\n",
    "#     y = y.reshape((-1))\n",
    "#     colors = rgb[y,x]\n",
    "#     # print(colors.shape)\n",
    "#     return colors.T\n",
    "\n",
    "def create_lseg_map_batch(img_save_dir, camera_translation, cs=0.05, gs=1000, depth_sample_rate=100):\n",
    "    mask_version = 1 # 0, 1\n",
    "\n",
    "    # crop_size = 480 # 480\n",
    "    # base_size = 520 # 520\n",
    "    # lang = \"door,chair,ground,ceiling,window,other\"\n",
    "    # labels = lang.split(\",\")\n",
    "\n",
    "    # loading models\n",
    "    # device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # print(device)\n",
    "    # clip_version = \"ViT-B/32\"\n",
    "    # clip_feat_dim = {'RN50': 1024, 'RN101': 512, 'RN50x4': 640, 'RN50x16': 768,\n",
    "    #                 'RN50x64': 1024, 'ViT-B/32': 512, 'ViT-B/16': 512, 'ViT-L/14': 768}[clip_version]\n",
    "    # print(\"Loading CLIP model...\")\n",
    "    # clip_model, preprocess = clip.load(clip_version, device = device)  # clip.available_models()\n",
    "    # clip_model.to(device).eval()\n",
    "    # # print(\"Clip model -\", clip_model.is_cuda())\n",
    "    # # x = torch.randn((5,5)).to(device=device)\n",
    "    # # print(\"Tensor device - \", type(x))\n",
    "    # # clip_model.eval()\n",
    "    # lang_token = clip.tokenize(labels)\n",
    "    # lang_token = lang_token.to(device)\n",
    "    # with torch.no_grad():\n",
    "    #     text_feats = clip_model.encode_text(lang_token)\n",
    "    #     text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
    "    # text_feats = text_feats.cpu().numpy()\n",
    "    # model = LSegEncNet(lang, arch_option=0,\n",
    "    #                     block_depth=0,\n",
    "    #                     activation='lrelu',\n",
    "    #                     crop_size=crop_size)\n",
    "    # model_state_dict = model.state_dict()\n",
    "    global data_root\n",
    "    # checkpoint_path = os.path.join(data_root, \"lseg/checkpoints/demo_e200.ckpt\")\n",
    "\n",
    "    # print(\"Loading lseg checkpoint\")\n",
    "    # pretrained_state_dict = torch.load(checkpoint_path, map_location = device)\n",
    "    # print(\"Lseg loaded\")\n",
    "    # pretrained_state_dict = {k.lstrip('net.'): v for k, v in pretrained_state_dict['state_dict'].items()}\n",
    "    # model_state_dict.update(pretrained_state_dict)\n",
    "    # model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "    # model.eval()\n",
    "    # model = model.cuda()\n",
    "\n",
    "    # norm_mean= [0.5, 0.5, 0.5]\n",
    "    # norm_std = [0.5, 0.5, 0.5]\n",
    "    # padding = [0.0] * 3\n",
    "    # transform = transforms.Compose(\n",
    "    #     [\n",
    "    #         transforms.ToTensor(),\n",
    "    #         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    print(f\"loading scene {img_save_dir}\")\n",
    "    rgb_dir = os.path.join(img_save_dir, \"rgb\")\n",
    "    depth_dir = os.path.join(img_save_dir, \"depth\")\n",
    "    pose_dir = os.path.join(img_save_dir, \"pose\")\n",
    "    # semantic_dir = os.path.join(img_save_dir, \"semantic\")\n",
    "    # obj2cls_path = os.path.join(img_save_dir, \"obj2cls_dict.txt\")\n",
    "\n",
    "    rgb_list = sorted(os.listdir(rgb_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    depth_list = sorted(os.listdir(depth_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "        x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    # pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "    #     x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    # semantic_list = sorted(os.listdir(semantic_dir), key=lambda x: int(\n",
    "    #     x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "    rgb_list = [os.path.join(rgb_dir, x) for x in rgb_list]\n",
    "    depth_list = [os.path.join(depth_dir, x) for x in depth_list]\n",
    "    pose_list = [os.path.join(pose_dir, x) for x in pose_list]\n",
    "    # semantic_list = [os.path.join(semantic_dir, x) for x in semantic_list]\n",
    "\n",
    "\n",
    "    map_save_dir = os.path.join(img_save_dir, \"map\")\n",
    "    os.makedirs(map_save_dir, exist_ok=True)\n",
    "    color_top_down_save_path = os.path.join(map_save_dir, f\"color_top_down_{mask_version}.npy\")\n",
    "    # gt_save_path = os.path.join(map_save_dir, f\"grid_{mask_version}_gt.npy\")\n",
    "    grid_save_path = os.path.join(map_save_dir, f\"grid_lseg_{mask_version}.npy\")\n",
    "    weight_save_path = os.path.join(map_save_dir, f\"weight_lseg_{mask_version}.npy\")\n",
    "    obstacles_save_path = os.path.join(map_save_dir, \"obstacles.npy\")\n",
    "\n",
    "    # obj2cls = load_obj2cls_dict(obj2cls_path)\n",
    "\n",
    "    # initialize a grid with zero position at the center\n",
    "    # color_top_down_height = (camera_height + 1) * np.ones((gs, gs), dtype=np.float32)\n",
    "    # color_top_down = np.zeros((gs, gs, 3), dtype=np.uint8)\n",
    "    # gt = np.zeros((gs, gs), dtype=np.int32)\n",
    "    # grid = np.zeros((gs, gs, clip_feat_dim), dtype=np.float32)\n",
    "    obstacles = np.ones((gs, gs), dtype=np.uint8)\n",
    "    weight = np.zeros((gs, gs), dtype=float)\n",
    "\n",
    "    # save_map(color_top_down_save_path, color_top_down)\n",
    "    # save_map(gt_save_path, gt)\n",
    "    # save_map(grid_save_path, grid)\n",
    "    # save_map(weight_save_path, weight)\n",
    "    # save_map(obstacles_save_path, obstacles)\n",
    "\n",
    "    tf_list = []\n",
    "    # data_iter = zip(rgb_list, depth_list, semantic_list, pose_list)\n",
    "    data_iter = zip(rgb_list, depth_list, pose_list)\n",
    "    pbar = tqdm(total=len(rgb_list))\n",
    "    # load all images and depths and poses\n",
    "    # print('here')\n",
    "    pc_combined = []\n",
    "    colors = []\n",
    "    count = 0\n",
    "    for data_sample in data_iter:\n",
    "        count += 1\n",
    "        # if count % 3 != 0:\n",
    "        #     continue\n",
    "        # rgb_path, depth_path, semantic_path, pose_path = data_sample\n",
    "        # if count < 20:\n",
    "        #     continue\n",
    "        # if count > 22:\n",
    "        #     break\n",
    "        \n",
    "        rgb_path, depth_path, pose_path = data_sample\n",
    "\n",
    "        \n",
    "        bgr = cv2.imread(rgb_path)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # read pose\n",
    "        # pos, rot = load_pose(pose_path)  # z backward, y upward, x to the right -> theirs\n",
    "        pos, rot = load_pose(pose_path)  # x front, y left, z upward -> ours\n",
    "\n",
    "        trans_cam = np.zeros((4,4))\n",
    "        trans_cam[0, 2] = 1\n",
    "        trans_cam[1, 0] = -1\n",
    "        trans_cam[2, 1] = -1\n",
    "\n",
    "        trans_cam[:3,3] = np.array(camera_translation)\n",
    "        trans_cam[3,3] = 1\n",
    "        # trans_cam\n",
    "\n",
    "        pose = np.eye(4)\n",
    "        pose[:3, :3] = rot\n",
    "        pose[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "        # pose = pose @ trans_cam\n",
    "\n",
    "        tf_list.append(pose)\n",
    "        if len(tf_list) == 1:\n",
    "            init_tf_inv = np.linalg.inv(tf_list[0])\n",
    "\n",
    "        tf = init_tf_inv @ pose @ trans_cam\n",
    "\n",
    "        # read depth\n",
    "        depth = load_depth(depth_path)\n",
    "        # print(f\"depth shape = {depth.shape}\")\n",
    "        # break\n",
    "        # read semantic\n",
    "        # semantic = load_semantic(semantic_path)\n",
    "        # semantic = cvt_obj_id_2_cls_id(semantic, obj2cls)\n",
    "\n",
    "        # pix_feats = get_lseg_feat(model, rgb, labels, transform, crop_size, base_size, norm_mean, norm_std)\n",
    "        \n",
    "        # transform all points to the global frame\n",
    "        pc, mask = depth2pc_realsense(depth)\n",
    "        shuffle_mask = np.arange(pc.shape[1]) \n",
    "        np.random.shuffle(shuffle_mask)\n",
    "        shuffle_mask = shuffle_mask[::depth_sample_rate]\n",
    "        mask = mask[shuffle_mask]\n",
    "        pc = pc[:, shuffle_mask]\n",
    "        pc = pc[:, mask]\n",
    "        pc_global = transform_pc(pc, tf)\n",
    "\n",
    "        color_local = get_color(rgb)\n",
    "        color_local = color_local[:, shuffle_mask]\n",
    "        color_local = color_local[:, mask]\n",
    "\n",
    "        if pc_combined == []:\n",
    "            pc_combined = pc_global\n",
    "            colors = color_local\n",
    "        else:\n",
    "            pc_combined = np.hstack((pc_combined, pc_global))\n",
    "            # print(color_local.shape, colors.shape)\n",
    "            colors = np.hstack((colors, color_local))\n",
    "\n",
    "\n",
    "        # rgb_cam_mat = get_real_cam_mat(rgb.shape[0], rgb.shape[1])\n",
    "        # # print(pix_feats.shape)\n",
    "        # # feat_cam_mat = get_real_cam_mat(pix_feats.shape[2], pix_feats.shape[3])\n",
    "\n",
    "        # # project all point cloud onto the ground\n",
    "        # for i, (p, p_local) in enumerate(zip(pc_global.T, pc.T)):\n",
    "        #     x, y = pos2grid_id(gs, cs, p[0], p[1])   # changed p[2] -> p[1], i.e, z -> y\n",
    "\n",
    "        #     # ignore points projected to outside of the map and points that are 0.5 higher than the camera (could be from the ceiling)\n",
    "        #     if x >= obstacles.shape[0] or y >= obstacles.shape[1] or \\\n",
    "        #         x < 0 or y < 0 or p_local[2] < -0.5:\n",
    "        #         continue\n",
    "\n",
    "        #     rgb_px, rgb_py, rgb_pz = project_point(rgb_cam_mat, p_local)\n",
    "        #     rgb_v = rgb[rgb_py, rgb_px, :]\n",
    "        #     # semantic_v = semantic[rgb_py, rgb_px]\n",
    "        #     # if semantic_v == 40:\n",
    "        #     #     semantic_v = -1\n",
    "            \n",
    "        #     # when the projected location is already assigned a color value before, overwrite if the current point has larger height\n",
    "        #     if p_local[2] < color_top_down_height[y, x]:\n",
    "        #         color_top_down[y, x] = rgb_v\n",
    "        #         color_top_down_height[y, x] = p_local[2]\n",
    "        #         # gt[y, x] = semantic_v\n",
    "\n",
    "        #     # average the visual embeddings if multiple points are projected to the same grid cell\n",
    "        #     # px, py, pz = project_point(feat_cam_mat, p_local)\n",
    "        #     # if not (px < 0 or py < 0 or px >= pix_feats.shape[3] or py >= pix_feats.shape[2]):\n",
    "        #     #     feat = pix_feats[0, :, py, px]\n",
    "        #     #     grid[y, x] = (grid[y, x] * weight[y, x] + feat) / (weight[y, x] + 1)\n",
    "        #     #     weight[y, x] += 1\n",
    "            \n",
    "        #     # build an obstacle map ignoring points on the floor (0 means occupied, 1 means free)\n",
    "        #     if p_local[2] > camera_height:\n",
    "        #         continue\n",
    "        #     obstacles[y, x] = 0\n",
    "        pbar.update(1)\n",
    "        if count == 75:\n",
    "            break\n",
    "        # count += 1\n",
    "        # if count == 500:\n",
    "        #     break\n",
    "\n",
    "    \n",
    "    # save_map(color_top_down_save_path, color_top_down)\n",
    "    # save_map(gt_save_path, gt)\n",
    "    # save_map(grid_save_path, grid)\n",
    "    # save_map(weight_save_path, weight)\n",
    "    # save_map(obstacles_save_path, obstacles)\n",
    "    # colors = colors.reshape((-1,3))\n",
    "    np.save(os.path.join(map_save_dir, \"pointcloud.npy\"), pc_combined)\n",
    "    np.save(os.path.join(map_save_dir, \"colors.npy\"), colors)\n",
    "    return pc_combined, colors\n",
    "\n",
    "\n",
    "# def get_lseg_feat(model: LSegEncNet, image: np.array, labels, transform, crop_size=480, \\\n",
    "#                  base_size=520, norm_mean=[0.5, 0.5, 0.5], norm_std=[0.5, 0.5, 0.5]):\n",
    "#     vis_image = image.copy()\n",
    "#     image = transform(image).unsqueeze(0).cuda()\n",
    "#     img = image[0].permute(1,2,0)\n",
    "#     img = img * 0.5 + 0.5\n",
    "    \n",
    "#     batch, _, h, w = image.size()\n",
    "#     stride_rate = 2.0/3.0\n",
    "#     stride = int(crop_size * stride_rate)\n",
    "\n",
    "#     long_size = base_size\n",
    "#     if h > w:\n",
    "#         height = long_size\n",
    "#         width = int(1.0 * w * long_size / h + 0.5)\n",
    "#         short_size = width\n",
    "#     else:\n",
    "#         width = long_size\n",
    "#         height = int(1.0 * h * long_size / w + 0.5)\n",
    "#         short_size = height\n",
    "\n",
    "\n",
    "#     cur_img = resize_image(image, height, width, **{'mode': 'bilinear', 'align_corners': True})\n",
    "\n",
    "#     if long_size <= crop_size:\n",
    "#         pad_img = pad_image(cur_img, norm_mean,\n",
    "#                             norm_std, crop_size)\n",
    "#         print(pad_img.shape)\n",
    "#         with torch.no_grad():\n",
    "#             outputs, logits = model(pad_img, labels)\n",
    "#         outputs = crop_image(outputs, 0, height, 0, width)\n",
    "#     else:\n",
    "#         if short_size < crop_size:\n",
    "#             # pad if needed\n",
    "#             pad_img = pad_image(cur_img, norm_mean,\n",
    "#                                 norm_std, crop_size)\n",
    "#         else:\n",
    "#             pad_img = cur_img\n",
    "#         _,_,ph,pw = pad_img.shape #.size()\n",
    "#         assert(ph >= height and pw >= width)\n",
    "#         h_grids = int(math.ceil(1.0 * (ph-crop_size)/stride)) + 1\n",
    "#         w_grids = int(math.ceil(1.0 * (pw-crop_size)/stride)) + 1\n",
    "#         with torch.cuda.device_of(image):\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = image.new().resize_(batch, model.out_c,ph,pw).zero_().cuda()\n",
    "#                 logits_outputs = image.new().resize_(batch, len(labels),ph,pw).zero_().cuda()\n",
    "#             count_norm = image.new().resize_(batch,1,ph,pw).zero_().cuda()\n",
    "#         # grid evaluation\n",
    "#         for idh in range(h_grids):\n",
    "#             for idw in range(w_grids):\n",
    "#                 h0 = idh * stride\n",
    "#                 w0 = idw * stride\n",
    "#                 h1 = min(h0 + crop_size, ph)\n",
    "#                 w1 = min(w0 + crop_size, pw)\n",
    "#                 crop_img = crop_image(pad_img, h0, h1, w0, w1)\n",
    "#                 # pad if needed\n",
    "#                 pad_crop_img = pad_image(crop_img, norm_mean,\n",
    "#                                             norm_std, crop_size)\n",
    "#                 with torch.no_grad():\n",
    "#                     output, logits = model(pad_crop_img, labels)\n",
    "#                 cropped = crop_image(output, 0, h1-h0, 0, w1-w0)\n",
    "#                 cropped_logits = crop_image(logits, 0, h1-h0, 0, w1-w0)\n",
    "#                 outputs[:,:,h0:h1,w0:w1] += cropped\n",
    "#                 logits_outputs[:,:,h0:h1,w0:w1] += cropped_logits\n",
    "#                 count_norm[:,:,h0:h1,w0:w1] += 1\n",
    "#         assert((count_norm==0).sum()==0)\n",
    "#         outputs = outputs / count_norm\n",
    "#         logits_outputs = logits_outputs / count_norm\n",
    "#         outputs = outputs[:,:,:height,:width]\n",
    "#         logits_outputs = logits_outputs[:,:,:height,:width]\n",
    "#     outputs = outputs.cpu()\n",
    "#     outputs = outputs.numpy() # B, D, H, W\n",
    "#     predicts = [torch.max(logit, 0)[1].cpu().numpy() for logit in logits_outputs]\n",
    "#     pred = predicts[0]\n",
    "\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2,3], [4,5,6]])[np.array([0,0]), np.array([1,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home2/laksh.nanwani/vlmaps-lab']\n",
      "/home2/laksh.nanwani/vlmaps-lab\n",
      "/scratch/laksh.nanwani/vlmaps_data/data_odom\n"
     ]
    }
   ],
   "source": [
    "root_dir = !pwd\n",
    "print(root_dir)\n",
    "root_dir = root_dir[0]\n",
    "print(root_dir)\n",
    "# !mkdir data\n",
    "# %cd data\n",
    "# !gdown 1wjuiVcO92Rqer5gLk-X7hINfe4PCMQmu\n",
    "# !pip install tqdm\n",
    "# !unzip -o 5LpN3gDmAk7_1.zip | tqdm --desc extracted --unit files --unit_scale --total `unzip -l 5LpN3gDmAk7_1.zip | tail -n 1 | xargs echo -n | cut -d' ' -f2` > /dev/null\n",
    "import os\n",
    "data_root = os.path.join(root_dir.split(\"home2\")[0], \"scratch/laksh.nanwani/vlmaps_data\")\n",
    "\n",
    "# checkpoint_dir = os.path.join(data_root, \"checkpoints\")\n",
    "folder_name = \"data_odom\"\n",
    "data_dir = os.path.join(data_root, folder_name)\n",
    "# data_dir = os.path.join(data_root, \"data_depth_issue\")\n",
    "\n",
    "\n",
    "# print(checkpoint_dir)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "# @markdown meters per cell size\n",
    "cs = 0.05 # @param {type: \"number\"}\n",
    "# @markdown map resolution (gs x gs)\n",
    "gs = 1000 # @param {type: \"integer\"}\n",
    "# @markdown camera height (used for filtering out points on the floor)\n",
    "# camera_height = 1.5 # @param {type: \"number\"}\n",
    "# 0.51 - base_link -> camera_link\n",
    "camera_translation = [0.2, -0.222, 0.512]\n",
    "# @markdown depth pixels subsample rate\n",
    "depth_sample_rate = 100 # @param {type: \"integer\"}\n",
    "# @markdown data where rgb, depth, pose are loaded and map are saved\n",
    "data_dir = data_dir # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scene /scratch/laksh.nanwani/vlmaps_data/data_odom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/274 [00:00<02:08,  2.13it/s]/tmp/ipykernel_3328356/4168299532.py:227: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if pc_combined == []:\n",
      " 27%|██▋       | 75/274 [00:29<01:17,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "pcd, colors = create_lseg_map_batch(data_dir, camera_translation=camera_translation, cs=cs, gs=gs, depth_sample_rate=depth_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 399606)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd.shape\n",
    "# colors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] GLFW Error: X11: The DISPLAY environment variable is missing\u001b[0;m\n",
      "\u001b[1;33m[Open3D WARNING] Failed to initialize GLFW\u001b[0;m\n",
      "\u001b[1;33m[Open3D WARNING] [DrawGeometries] Failed creating OpenGL window.\u001b[0;m\n"
     ]
    }
   ],
   "source": [
    "PC = o3d.geometry.PointCloud()\n",
    "PC.points = o3d.utility.Vector3dVector(pcd.T)\n",
    "o3d.visualization.draw_geometries([PC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4411e80da8008d6a0c6bf425013b61b6956630bc6111cfb4a65d9f725bf5dc68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
